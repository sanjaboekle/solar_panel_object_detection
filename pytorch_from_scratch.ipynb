{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Import custom dataset class\n",
    "from utils.dataset import SolarPanelDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Kasmi_2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class SolarPanelDataset(Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        # root directory for the dataset\n",
    "        self.root = root\n",
    "        # transformations to be applied on the images and masks\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # os.path.join is used to combine parts of a path in a platform-independent way\n",
    "        # os.listdir returns a list of all files in the specified directory\n",
    "        # sorted() is used to sort the list of files, ensuring that the images and masks align\n",
    "\n",
    "        # load all image files from the \"images\" directory inside the root directory\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"img\"))))\n",
    "        # load all mask files from the \"masks\" directory inside the root directory\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"mask\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"img\", self.imgs[idx])\n",
    "        img = read_image(img_path, mode=ImageReadMode.RGB)\n",
    "\n",
    "        mask_path = os.path.join(self.root, \"mask\", self.masks[idx])\n",
    "        mask = read_image(mask_path, mode=ImageReadMode.RGB)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        boxes = masks_to_boxes(masks)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        img = tv_tensors.Image(img)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(\n",
    "            boxes, format=\"XYWH\", canvas_size=F.get_size(img)\n",
    "        )\n",
    "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            if isinstance(img, Image.Image) or isinstance(img, np.ndarray):\n",
    "                img = self.transforms(img)  # earlier version had (img, target)\n",
    "\n",
    "        return img, mask, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Val-Split & Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is defining a series of transformations that will be applied to the images in your dataset.\n",
    "\n",
    "`transforms.Compose` is a function from the PyTorch library that composes several transformations together. It takes a list of transformations and applies them in the order they are provided.\n",
    "\n",
    "Here are the transformations being applied:\n",
    "\n",
    "1. `transforms.ToTensor()`: This transformation converts a PIL Image or a numpy.ndarray to a PyTorch tensor. It also scales the image's pixel intensity values in the range 0-255 to a float in the range 0.0-1.0.\n",
    "\n",
    "2. `transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))`: This transformation normalizes a tensor image with mean and standard deviation. Given mean `(M1,...,Mn)` and std `(S1,..,Sn)` for `n` channels, this transform will normalize each channel of the input `torch.*Tensor` i.e., `output[channel] = (input[channel] - mean[channel]) / std[channel]`. Here, it's normalizing the image by setting the mean and standard deviation for all three color channels (Red, Green, Blue) to 0.5. This will shift the pixel intensity values from a range of [0,1] to [-1,1].\n",
    "\n",
    "These transformations are commonly used when working with image data in PyTorch. They help to standardize the input data, making it easier for the neural network to learn from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "# Define the root directory\n",
    "root_dir = f\"data/{dataset_name}\"\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = SolarPanelDataset(root_dir, transform)\n",
    "\n",
    "# Define the sizes of your splits\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several common transformations used in PyTorch, especially when dealing with image data. Here are a few:\n",
    "\n",
    "1. `transforms.Resize(size)`: Resizes the input image to the given size.\n",
    "\n",
    "2. `transforms.CenterCrop(size)`: Crops the image at the center to have a size equal to the provided size.\n",
    "\n",
    "3. `transforms.RandomCrop(size)`: Crops the image at a random location to have a size equal to the provided size.\n",
    "\n",
    "4. `transforms.RandomHorizontalFlip()`: Horizontally flips the image with a given probability (default is 0.5).\n",
    "\n",
    "5. `transforms.RandomVerticalFlip()`: Vertically flips the image with a given probability (default is 0.5).\n",
    "\n",
    "6. `transforms.RandomRotation(degrees)`: Rotates the image by a random angle within the given range of degrees.\n",
    "\n",
    "7. `transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)`: Randomly changes the brightness, contrast, and saturation of an image.\n",
    "\n",
    "8. `transforms.Grayscale(num_output_channels=1)`: Converts the image to grayscale.\n",
    "\n",
    "9. `transforms.Pad(padding, fill=0, padding_mode='constant')`: Pads the image on all sides with the specified padding value.\n",
    "\n",
    "10. `transforms.ToPILImage()`: Converts a tensor or an ndarray to PIL Image.\n",
    "\n",
    "These transformations can be used to augment the data, which can help improve the performance of the model by providing more varied data for training. The appropriate transformations to use depend on the specific task and the nature of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of the train_dataset sample is: <class 'tuple'>\n",
      "The number of items in the sample is: 3\n",
      "The type of the image is: <class 'torchvision.tv_tensors._image.Image'>\n",
      "The size of the image is: torch.Size([3, 400, 400])\n",
      "The mask file looks like this: tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)\n",
      "The keys in the target dictionary are: dict_keys(['boxes', 'masks', 'labels', 'image_id', 'area', 'iscrowd'])\n",
      "The values in the target dictionary are: dict_values([BoundingBoxes([[164., 169., 232., 223.],\n",
      "               [164., 169., 232., 223.],\n",
      "               [164., 169., 232., 223.]], format=BoundingBoxFormat.XYXY, canvas_size=[400, 400]), Mask([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "      [[0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "      [[0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8), tensor([1]), 283, tensor([3672., 3672., 3672.]), tensor([0])])\n",
      "The target dictionary is: {'boxes': BoundingBoxes([[164., 169., 232., 223.],\n",
      "               [164., 169., 232., 223.],\n",
      "               [164., 169., 232., 223.]], format=BoundingBoxFormat.XYXY, canvas_size=[400, 400]), 'masks': Mask([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "      [[0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "      [[0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0],\n",
      "       [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8), 'labels': tensor([1]), 'image_id': 283, 'area': tensor([3672., 3672., 3672.]), 'iscrowd': tensor([0])}\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "print(\n",
    "    f\"The type of the train_dataset sample is: {type(sample)}\"\n",
    ")  # Print the type of the sample\n",
    "print(\n",
    "    f\"The number of items in the sample is: {len(sample)}\"\n",
    ")  # Print the number of items in the sample\n",
    "\n",
    "# If the sample is a tuple containing an image and a label\n",
    "image, mask, target = sample\n",
    "print(f\"The type of the image is: {type(image)}\")  # Print the type of the image\n",
    "print(f\"The size of the image is: {image.size()}\")  # Print the size of the image\n",
    "\n",
    "print(f\"The mask file looks like this: {mask}\")  # Print the label\n",
    "\n",
    "# Print the keys in the dictionary\n",
    "print(f\"The keys in the target dictionary are: {target.keys()}\")\n",
    "\n",
    "# Print the values in the dictionary\n",
    "print(f\"The values in the target dictionary are: {target.values()}\")\n",
    "\n",
    "# Print the entire dictionary\n",
    "print(f\"The target dictionary is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.dataset.SolarPanelDataset at 0x1581363d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trainset has 9312 images. The validation set has 1995 images. The testset has 1996 images.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"The trainset has {train_size} images. The validation set has {val_size} images. The testset has {test_size} images.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x14feb6750>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 1/582\n",
      "Loss: 0.7551432251930237\n",
      "Epoch 1/10, Batch 101/582\n",
      "Loss: -67767066624.0\n",
      "Epoch 1/10, Batch 201/582\n",
      "Loss: -7825545756672.0\n",
      "Epoch 1/10, Batch 301/582\n",
      "Loss: -95595880513536.0\n",
      "Epoch 1/10, Batch 401/582\n",
      "Loss: -665795025698816.0\n",
      "Epoch 1/10, Batch 501/582\n",
      "Loss: -1616000504037376.0\n",
      "Epoch 2/10, Batch 1/582\n",
      "Loss: -4361068721733632.0\n",
      "Epoch 2/10, Batch 101/582\n",
      "Loss: -1.0567780990451712e+16\n",
      "Epoch 2/10, Batch 201/582\n",
      "Loss: -2.330561138707661e+16\n",
      "Epoch 2/10, Batch 301/582\n",
      "Loss: -3.401253535927501e+16\n",
      "Epoch 2/10, Batch 401/582\n",
      "Loss: -5.505738763088691e+16\n",
      "Epoch 2/10, Batch 501/582\n",
      "Loss: -8.561976072889958e+16\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Define a Model\n",
    "class SolarPanelDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SolarPanelDetector, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 97 * 97, 120)  # Adjusted to match the actual size\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3 * 400 * 400)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 97 * 97)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x.view(-1, 3, 400, 400)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = SolarPanelDetector()\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 2\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels, _ = data\n",
    "\n",
    "        # Convert inputs and labels to float tensors\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Batch {i + 1}/{len(train_loader)}\")\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(\"model\"):\n",
    "    os.makedirs(\"model\")\n",
    "\n",
    "version = 0\n",
    "filename = f\"model/model_{dataset_name}.pth\"\n",
    "\n",
    "# Check if the file exists and increment version number until a unique filename is found\n",
    "while os.path.exists(filename):\n",
    "    version += 1\n",
    "    filename = f\"model_{dataset_name}_v{version}.pth\"\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[ 84,  91,  91,  ..., 195, 195, 195],\n",
      "          [ 89,  92,  91,  ..., 195, 195, 195],\n",
      "          [ 91,  92,  91,  ..., 195, 195, 195],\n",
      "          ...,\n",
      "          [128, 186, 215,  ..., 166, 166, 166],\n",
      "          [174, 215, 243,  ..., 174, 166, 158],\n",
      "          [226, 251, 251,  ..., 166, 166, 158]],\n",
      "\n",
      "         [[ 84,  91,  91,  ..., 195, 195, 195],\n",
      "          [ 89,  96,  91,  ..., 195, 195, 195],\n",
      "          [ 91,  96,  91,  ..., 195, 195, 195],\n",
      "          ...,\n",
      "          [139, 187, 215,  ..., 166, 166, 166],\n",
      "          [174, 215, 243,  ..., 174, 166, 158],\n",
      "          [226, 251, 251,  ..., 166, 166, 158]],\n",
      "\n",
      "         [[ 74,  84,  84,  ..., 179, 179, 179],\n",
      "          [ 74,  80,  84,  ..., 179, 179, 179],\n",
      "          [ 84,  80,  84,  ..., 179, 179, 179],\n",
      "          ...,\n",
      "          [124, 179, 203,  ..., 159, 159, 159],\n",
      "          [163, 203, 235,  ..., 163, 159, 154],\n",
      "          [214, 235, 235,  ..., 159, 159, 154]]],\n",
      "\n",
      "\n",
      "        [[[215, 251, 255,  ...,  28,  36,  56],\n",
      "          [227, 215, 215,  ...,  40,  40,  56],\n",
      "          [235, 215, 215,  ...,  56,  48,  68],\n",
      "          ...,\n",
      "          [  8,   0,   0,  ...,  28,  36,  76],\n",
      "          [ 12,   8,   4,  ...,  12,  24,  56],\n",
      "          [ 17,  17,  17,  ...,  12,   8,  36]],\n",
      "\n",
      "         [[215, 243, 252,  ...,  28,  28,  56],\n",
      "          [227, 215, 215,  ...,  36,  36,  56],\n",
      "          [239, 215, 215,  ...,  56,  51,  67],\n",
      "          ...,\n",
      "          [  8,   0,   0,  ...,  28,  44,  68],\n",
      "          [ 12,   8,   4,  ...,  20,  20,  52],\n",
      "          [ 17,  12,  12,  ...,  12,   8,  44]],\n",
      "\n",
      "         [[199, 227, 237,  ...,  28,  36,  59],\n",
      "          [211, 199, 199,  ...,  40,  40,  59],\n",
      "          [219, 199, 199,  ...,  59,  51,  67],\n",
      "          ...,\n",
      "          [  0,   0,   0,  ...,  28,  32,  60],\n",
      "          [  4,   0,   0,  ...,  12,  16,  40],\n",
      "          [ 12,   9,   9,  ...,   4,   0,  32]]],\n",
      "\n",
      "\n",
      "        [[[ 56,  58,  52,  ...,  25,  40,  33],\n",
      "          [ 58,  58,  56,  ...,  25,  40,  32],\n",
      "          [ 67,  72,  72,  ...,  25,  25,  25],\n",
      "          ...,\n",
      "          [ 16,  24,  16,  ...,  17,  28,  24],\n",
      "          [ 16,  24,  24,  ...,  17,  17,  17],\n",
      "          [ 24,  33,  33,  ...,  17,  17,  17]],\n",
      "\n",
      "         [[ 72,  67,  60,  ...,  20,  24,  34],\n",
      "          [ 76,  76,  72,  ...,  20,  24,  28],\n",
      "          [ 81,  84,  84,  ...,  20,  20,  20],\n",
      "          ...,\n",
      "          [ 27,  24,  16,  ...,  17,  24,  24],\n",
      "          [ 27,  24,  24,  ...,  17,  17,  17],\n",
      "          [ 28,  34,  34,  ...,  17,  17,  17]],\n",
      "\n",
      "         [[ 52,  58,  44,  ...,  33,  40,  40],\n",
      "          [ 56,  56,  52,  ...,  33,  40,  34],\n",
      "          [ 64,  68,  68,  ...,  33,  33,  33],\n",
      "          ...,\n",
      "          [ 28,  32,  20,  ...,  12,  20,  16],\n",
      "          [ 28,  32,  32,  ...,  12,  12,  12],\n",
      "          [ 32,  40,  40,  ...,  12,  12,  12]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 89,  89,  88,  ...,  44,  44,  57],\n",
      "          [ 89,  89,  88,  ...,  57,  57,  68],\n",
      "          [ 89,  89,  89,  ...,  64,  65,  72],\n",
      "          ...,\n",
      "          [171, 146, 151,  ...,  96, 100, 104],\n",
      "          [171, 151, 151,  ..., 108, 112, 112],\n",
      "          [159, 151, 151,  ..., 120, 120, 120]],\n",
      "\n",
      "         [[ 98,  98, 100,  ...,  60,  60,  73],\n",
      "          [ 98,  98, 100,  ...,  73,  73,  84],\n",
      "          [ 98,  98,  98,  ...,  84,  88,  92],\n",
      "          ...,\n",
      "          [171, 146, 155,  ..., 100, 104, 106],\n",
      "          [171, 155, 155,  ..., 112, 116, 116],\n",
      "          [162, 155, 155,  ..., 124, 124, 124]],\n",
      "\n",
      "         [[ 81,  81,  76,  ...,  40,  40,  51],\n",
      "          [ 81,  81,  76,  ...,  51,  51,  60],\n",
      "          [ 81,  81,  81,  ...,  56,  60,  64],\n",
      "          ...,\n",
      "          [159, 139, 143,  ..., 100, 104, 106],\n",
      "          [159, 143, 143,  ..., 112, 115, 115],\n",
      "          [155, 143, 143,  ..., 116, 116, 116]]],\n",
      "\n",
      "\n",
      "        [[[ 32,  28,  24,  ...,  92,  81,  44],\n",
      "          [ 28,  20,  16,  ...,  57,  52,  16],\n",
      "          [  9,  12,   9,  ...,  12,  12,   4],\n",
      "          ...,\n",
      "          [ 96,  89,  91,  ..., 175, 175, 179],\n",
      "          [ 89,  89,  91,  ..., 175, 175, 183],\n",
      "          [ 89,  98,  98,  ..., 175, 175, 179]],\n",
      "\n",
      "         [[ 33,  20,  25,  ...,  92,  80,  48],\n",
      "          [ 20,  20,  16,  ...,  57,  52,  16],\n",
      "          [  2,  12,   2,  ...,  12,  12,   0],\n",
      "          ...,\n",
      "          [ 96,  89,  88,  ..., 135, 135, 143],\n",
      "          [ 89,  89,  88,  ..., 135, 135, 147],\n",
      "          [ 89,  97,  97,  ..., 135, 135, 143]],\n",
      "\n",
      "         [[ 26,  20,  17,  ...,  76,  73,  36],\n",
      "          [ 20,  12,   8,  ...,  52,  44,   8],\n",
      "          [  1,   4,   1,  ...,   4,   4,   0],\n",
      "          ...,\n",
      "          [ 84,  81,  91,  ..., 100, 100, 104],\n",
      "          [ 81,  81,  91,  ..., 100, 100, 108],\n",
      "          [ 81,  89,  89,  ..., 100, 100, 104]]],\n",
      "\n",
      "\n",
      "        [[[ 35,  35,  35,  ...,  35,  35,  39],\n",
      "          [ 20,  20,  20,  ...,  54,  54,  54],\n",
      "          [ 12,  12,  20,  ...,  62,  68,  68],\n",
      "          ...,\n",
      "          [108, 112, 112,  ..., 137, 137, 140],\n",
      "          [108, 112, 112,  ..., 154, 154, 164],\n",
      "          [108, 112, 112,  ..., 108, 112, 128]],\n",
      "\n",
      "         [[ 42,  35,  50,  ...,  50,  50,  54],\n",
      "          [ 31,  31,  31,  ...,  74,  74,  74],\n",
      "          [ 27,  20,  31,  ...,  81,  93,  93],\n",
      "          ...,\n",
      "          [108, 112, 112,  ..., 128, 128, 136],\n",
      "          [108, 112, 112,  ..., 138, 138, 156],\n",
      "          [108, 112, 112,  ...,  99, 102, 116]],\n",
      "\n",
      "         [[ 35,  31,  35,  ...,  31,  31,  35],\n",
      "          [ 20,  20,  20,  ...,  54,  54,  54],\n",
      "          [ 12,  20,  20,  ...,  63,  68,  68],\n",
      "          ...,\n",
      "          [104, 108, 108,  ..., 112, 112, 120],\n",
      "          [104, 108, 108,  ..., 122, 122, 140],\n",
      "          [104, 108, 108,  ...,  82,  85, 102]]]], dtype=torch.uint8), tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]]], dtype=torch.uint8), {'boxes': tensor([[[169., 160., 218., 228.],\n",
      "         [169., 160., 218., 228.],\n",
      "         [169., 160., 218., 228.]],\n",
      "\n",
      "        [[174., 174., 230., 230.],\n",
      "         [174., 174., 230., 230.],\n",
      "         [174., 174., 230., 230.]],\n",
      "\n",
      "        [[148., 166., 238., 245.],\n",
      "         [148., 166., 238., 245.],\n",
      "         [148., 166., 238., 245.]],\n",
      "\n",
      "        [[183.,  66., 312., 229.],\n",
      "         [183.,  66., 312., 229.],\n",
      "         [183.,  66., 312., 229.]],\n",
      "\n",
      "        [[164., 174., 229., 233.],\n",
      "         [164., 174., 229., 233.],\n",
      "         [164., 174., 229., 233.]],\n",
      "\n",
      "        [[171., 168., 232., 224.],\n",
      "         [171., 168., 232., 224.],\n",
      "         [171., 168., 232., 224.]],\n",
      "\n",
      "        [[143., 138., 254., 300.],\n",
      "         [143., 138., 254., 300.],\n",
      "         [143., 138., 254., 300.]],\n",
      "\n",
      "        [[160., 185., 233., 223.],\n",
      "         [160., 185., 233., 223.],\n",
      "         [160., 185., 233., 223.]],\n",
      "\n",
      "        [[165., 182., 233., 220.],\n",
      "         [165., 182., 233., 220.],\n",
      "         [165., 182., 233., 220.]],\n",
      "\n",
      "        [[171., 184., 254., 223.],\n",
      "         [171., 184., 254., 223.],\n",
      "         [171., 184., 254., 223.]],\n",
      "\n",
      "        [[186., 172., 205., 223.],\n",
      "         [186., 172., 205., 223.],\n",
      "         [186., 172., 205., 223.]],\n",
      "\n",
      "        [[171., 174., 243., 222.],\n",
      "         [171., 174., 243., 222.],\n",
      "         [171., 174., 243., 222.]],\n",
      "\n",
      "        [[178., 173., 216., 223.],\n",
      "         [178., 173., 216., 223.],\n",
      "         [178., 173., 216., 223.]],\n",
      "\n",
      "        [[154., 174., 213., 223.],\n",
      "         [154., 174., 213., 223.],\n",
      "         [154., 174., 213., 223.]],\n",
      "\n",
      "        [[124., 139., 240., 237.],\n",
      "         [124., 139., 240., 237.],\n",
      "         [124., 139., 240., 237.]],\n",
      "\n",
      "        [[190., 157., 210., 239.],\n",
      "         [190., 157., 210., 239.],\n",
      "         [190., 157., 210., 239.]]]), 'masks': tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]]], dtype=torch.uint8), 'labels': tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]]), 'image_id': tensor([ 4083,  1647,  4092,  3291,  2432,  7597, 10411,  2100,  3614, 12402,\n",
      "         1514,   607, 12344,  7542, 10460, 12082]), 'area': tensor([[ 3332.,  3332.,  3332.],\n",
      "        [ 3136.,  3136.,  3136.],\n",
      "        [ 7110.,  7110.,  7110.],\n",
      "        [21027., 21027., 21027.],\n",
      "        [ 3835.,  3835.,  3835.],\n",
      "        [ 3416.,  3416.,  3416.],\n",
      "        [17982., 17982., 17982.],\n",
      "        [ 2774.,  2774.,  2774.],\n",
      "        [ 2584.,  2584.,  2584.],\n",
      "        [ 3237.,  3237.,  3237.],\n",
      "        [  969.,   969.,   969.],\n",
      "        [ 3456.,  3456.,  3456.],\n",
      "        [ 1900.,  1900.,  1900.],\n",
      "        [ 2891.,  2891.,  2891.],\n",
      "        [11368., 11368., 11368.],\n",
      "        [ 1640.,  1640.,  1640.]]), 'iscrowd': tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])}]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        print(data)  # Temporarily print data to inspect its structure\n",
    "        break  # Break after the first batch to avoid printing too much data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, jaccard_score\n",
    "\n",
    "\n",
    "def evaluate(predictions, targets):\n",
    "    # Extract the relevant data from the targets dictionary\n",
    "    labels = targets[\"labels\"].numpy()\n",
    "\n",
    "    # Flatten labels and predictions\n",
    "    labels = labels.flatten()\n",
    "    predictions = (predictions > 0.5).numpy().flatten()\n",
    "\n",
    "    # Check that labels and predictions have the same total size\n",
    "    assert (\n",
    "        labels.size == predictions.size\n",
    "    ), \"labels and predictions must have the same total size\"\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(labels, predictions, average=\"weighted\")\n",
    "    recall = recall_score(labels, predictions, average=\"weighted\")\n",
    "    IoU = jaccard_score(labels, predictions, average=\"weighted\")\n",
    "\n",
    "    return precision, recall, IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "labels and predictions must have the same total size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Evaluate the model's performance on the validation set\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m precision, recall, IoU \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[61], line 14\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(predictions, targets)\u001b[0m\n\u001b[1;32m     10\u001b[0m predictions \u001b[38;5;241m=\u001b[39m (predictions \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Check that labels and predictions have the same total size\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m---> 14\u001b[0m     labels\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m predictions\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m     15\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels and predictions must have the same total size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[1;32m     18\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(labels, predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: labels and predictions must have the same total size"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks, targets in val_loader:\n",
    "        images = images.float()  # Convert images to float\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Evaluate the model's performance on the validation set\n",
    "        precision, recall, IoU = evaluate(predictions, targets)\n",
    "        print(f\"Precision: {precision.item()}\")\n",
    "        print(f\"Recall: {recall.item()}\")\n",
    "        print(f\"IoU: {IoU.item()}\")\n",
    "\n",
    "        # Make predictions on new images\n",
    "        new_image = torch.randn((1, 3, 224, 224))\n",
    "        prediction = model(new_image)\n",
    "\n",
    "        # Apply post-processing steps like non-maximal suppression (NMS) to refine the predictions\n",
    "        refined_prediction = refine_prediction(prediction)\n",
    "\n",
    "        # Visualize the predicted mask\n",
    "        visualize_mask(refined_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
