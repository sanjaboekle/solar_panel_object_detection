# PanelVision - deep learning object detection of solar panels on satellite imagery



# Notes
These notes have the sole purpose of giving us some guidance when we want to eventually clean up this mess at some point. They mainly describe the different folders in this directory.



## Folder structure


### data
There are two different raw data sets in this folder. 

The kasmi_solar folder contains data from this paper:
https://www.nature.com/articles/s41597-023-01951-4

The maxar_solar folder contains data from this paper:
https://www.nature.com/articles/s41597-023-02539-8

We ended up using the kasmi_solar data, because it's a bigger and way cleaner data set. We used the "google" picture set of the kasmi_solar data, because we will be working with Google Map images for our app.


### datasets
This folder contains the train/test-splitted pictures and their labels (bounding boxes for detection or masks for segmentation), as well as some images for testing. 
It separates the kasmi_solar data into a detection and a segmentation folder.
Note that the labels are in the YOLO-format. Find more info on this in the ultralytics documentation.

The structure looks like:
datasets
- maxar_solar
-- train
-- valid
-- test


### downloaded_models
This folder contains the models, model results and model weights generated by and downloaded from our model_training scripts (on Google Drive, see down below).


### runs
When we run the streamlit app (app.py), YOLO saves its model runs in this 'runs' folder.


### streamlit_app
This contains the up-to-date app coded by Sanja. Super cool stuff. Run it with the command "streamlit run app.py".


### important files
moritz.ipynb and sanja.ipynb are our personal play-around files. 

moritz-ipynb:
- data cleaning
- convert masks to segmentation labels and further to bounding boxes
- train-test split

model_training_segmentation:
- Sanja's new approach to image segmentation



## Google Drive

To access Google GPU machines, we performed model training using Google Colab. 
Therefore, we set up a Google Drive account (solar: solar.panel.object.detection@gmail.com), to which we uploaded the data from the datasets folder, as well as the configuration file (.yaml) that tells YOLO where to find the data and labels. It also contains the model_training scripts, the downloaded YOLO pre-trained models as well as the model training results.

Google Drive folder - Solar_panel_detection:
https://drive.google.com/drive/folders/1hJlfeLodIpOZFFxZ3O_8DqEa_sGRqvK5?usp=drive_link

In the model_training script we mounted our Google Drive ("drive.mount('/content/drive')") and downloaded the data, yaml file and models from there. 



## To-Do

We still have to figure out how to publish this project. 
The main problem is that we cannot upload gigabytes of data to GitHub. At the same time, we cannot train our models locally, because it takes forever. So perhaps we can publish the Google Drive account with its containing data? That way we risk leaking some personal data somewhere.. If we don't do that, we can 




